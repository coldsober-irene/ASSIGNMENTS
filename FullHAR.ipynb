{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RJvcYBv8lNCdFcpuVsdd5AmMwV3etDx2",
      "authorship_tag": "ABX9TyNEWO9jNF/+Ok93hZVb06jM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/ASSIGNMENTS/blob/main/FullHAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8chyVBeuysnl",
        "outputId": "c96287be-e00d-4ef1-d626-92a34af33918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##combined codes"
      ],
      "metadata": {
        "id": "m2CzbuJoQhbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGVc_GIPQgYo"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from moviepy.editor import *\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from keras.layers import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, ConvLSTM2D, Dropout\n",
        "%matplotlib inline\n",
        "\n",
        "samples = 20\n",
        "rate = 7\n",
        "\n",
        "class Sampling:\n",
        "  count = 0\n",
        "  def __init__(self, video_path, map_file, sampling_type = 'uniform', val_sampling = False,\n",
        "               ref_mean=[0.07, 0.07, 0.07], ref_std=[0.1, 0.09, 0.08], enhance_img = False, **kwargs):\n",
        "    \"\"\"save_feature_dir: path for storing extracted features for future use\"\"\"\n",
        "    self.data_path = video_path\n",
        "    self.map_file = map_file\n",
        "    self.sampling_type = sampling_type\n",
        "    self.is_valSampling = val_sampling\n",
        "    self.mean = ref_mean\n",
        "    self.std = ref_std\n",
        "    self.enhance_img = enhance_img\n",
        "    # ALL FEATURES OBTAINED FROM ENTIRE DATASETS\n",
        "    self.obtained_features = []\n",
        "    self.labels = []\n",
        "\n",
        "    # GETTING MAPPING\n",
        "    self.maps = {}\n",
        "    with open(self.map_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "          parts = line.split()\n",
        "          if self.is_valSampling:\n",
        "            self.maps[parts[-1]] = int(parts[1])\n",
        "          else:\n",
        "            self.maps[parts[1]] = int(parts[0])\n",
        "    print(f'mapping: {self.maps}')\n",
        "    # SAMPLERS\n",
        "    self.TrainD_sampler = self.Sampling_Training(data_path = self.data_path,\n",
        "                                                 mapping = self.maps,\n",
        "                                                 labels_list = self.labels)\n",
        "    self.ValidationD_sampler = self.Sampling_Validation(data_path = self.data_path,\n",
        "                                                        mapping = self.maps,\n",
        "                                                labels_list = self.labels )\n",
        "\n",
        "    # FRAME SAMPLING\n",
        "    if val_sampling:\n",
        "      self.ValidationD_sampler.Sample(sampling_processor = self.sample)\n",
        "    else:\n",
        "      self.TrainD_sampler.Sample(sampling_processor = self.sample)\n",
        "\n",
        "\n",
        "    # SAVE EXTRACTED FEATURE INTO THE FILE FOR FUTURE USE\n",
        "    self.saveFeatures(destination_dir = kwargs['save_feature_dir'])\n",
        "    print(\"EXTRACTED FEATURE ARE SAVED SUCCESSFULLY!\")\n",
        "\n",
        "  def UniformSampling(self, cap, sample_rate, frameCount):\n",
        "    for i in range(0, frameCount, sample_rate):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "        if self.enhance_img:\n",
        "          return self.enhance_image(input_frame = frame)\n",
        "        return frame\n",
        "\n",
        "  def RandomSampling(self, cap,num_samples, frameCount):\n",
        "    sampled_indices = random.sample(range(frameCount), num_samples)\n",
        "    # ORIGINAL VS ENHANCED FRAME\n",
        "    frames_sampled_forPlot = {}\n",
        "    for i in sampled_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        frames_sampled_forPlot['original frame'] = frame\n",
        "        if ret:\n",
        "          if self.enhance_img:\n",
        "            enhanced = self.enhance_image(input_frame = frame)\n",
        "            frames_sampled_forPlot['enhanced frame'] = enhanced\n",
        "            x = lambda: display_frames(frames = frames_sampled_forPlot, title = 'Original frame vs Enhanced frame')\n",
        "            x()\n",
        "            return enhanced\n",
        "          return frame\n",
        "\n",
        "  def sample(self, cap, **kwargs):\n",
        "    # CREATE EXTRACTOR OBJECT\n",
        "    Extractor = Feature_extract(sampled_type = self.sampling_type)\n",
        "    Sampling.count += 1\n",
        "    if self.sampling_type == 'uniform':\n",
        "      sample_frame = self.UniformSampling(cap = cap, frameCount=kwargs['frame_count'], sample_rate = kwargs['sample_rate'])\n",
        "      kwargs['frames_sampled'].append(sample_frame)\n",
        "\n",
        "    elif self.sampling_type == 'random':\n",
        "      sample_frame = self.RandomSampling(cap = cap, frameCount=kwargs['frame_count'], num_samples=kwargs['num_samples'])\n",
        "      kwargs['frames_sampled'].append(sample_frame)\n",
        "\n",
        "    # EXTRACT FEATURE FROM THE FRAMES OF EACH VIDEO\n",
        "    features_obtained = Extractor.features(frames = kwargs['frames_sampled'], ref_mean = self.mean, ref_std = self.std)\n",
        "    self.obtained_features.append(features_obtained)\n",
        "    print('Constructor created!')\n",
        "\n",
        "  def enhance_image(self, input_frame, gamma=0.35, kernel_size=3):\n",
        "    # Apply gamma correction\n",
        "    gamma_corrected = np.power(input_frame / 255.0, gamma) * 255.0\n",
        "    gamma_corrected = gamma_corrected.astype(np.uint8)\n",
        "\n",
        "    # Return the gamma_corrected image\n",
        "    return gamma_corrected\n",
        "    print(\"FEATURE EXTRACTION AND SAVING IS DONE!!!\")\n",
        "\n",
        "  def saveFeatures(self, destination_dir):\n",
        "    # CREATE VSTACK ARRAY OF ALL FEATURES EXTRACTED\n",
        "    all_features = np.vstack(self.obtained_features)\n",
        "    labels = np.array(self.labels)\n",
        "\n",
        "    # SAVED THE EXTRACTED FEATURES and their corresponding labels FOR FUTURE USE\n",
        "    os.makedirs(destination_dir, exist_ok = True)\n",
        "\n",
        "    np.save(os.path.join(destination_dir,f'features{Sampling.count}.npy'), all_features)\n",
        "    np.save(os.path.join(destination_dir,f'labels{Sampling.count}.npy'), labels)\n",
        "\n",
        "\n",
        "  class Sampling_Training:\n",
        "    def __init__(self, data_path, mapping:dict, labels_list:list = None):\n",
        "      self.data_path = data_path\n",
        "      self.maps = mapping\n",
        "      # subfolders\n",
        "      self.activities = os.listdir(self.data_path)\n",
        "      # EXTRACTED FEATURES FROM ALL THE VIDEOS\n",
        "      self.obtained_features = []\n",
        "      # LABELS OF THE EXTRACTED FEATURES\n",
        "      self.labels_list = labels_list\n",
        "\n",
        "    def Sample(self,sampling_processor, sample_rate = 5, num_samples = 10):\n",
        "      # Loop through each activity\n",
        "      for activity in self.activities:\n",
        "          activity_folder = os.path.join(self.data_path, activity)\n",
        "\n",
        "          # Loop through video files in the activity folder\n",
        "          for video_file in os.listdir(activity_folder):\n",
        "            if '.mp4' in video_file:\n",
        "              frames_sampled = []\n",
        "              video_path = os.path.join(activity_folder, video_file)\n",
        "              cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "              frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "              # PROCESSOR FOR FINISHING THE TASK\n",
        "              sampling_processor(cap = cap, frame_count = frame_count,\n",
        "                  sample_rate = sample_rate, frames_sampled = frames_sampled\n",
        "                                 , num_samples = num_samples)\n",
        "              # POPULATE THE LABEL CORRESPONDING TO THE CURRENT VIDEO\n",
        "              if self.labels_list:\n",
        "                self.labels_list.append(self.maps[activity])\n",
        "\n",
        "  class Sampling_Validation:\n",
        "    def __init__(self, data_path,mapping:dict, labels_list:list = None):\n",
        "      self.data_path = data_path\n",
        "      self.maps = mapping\n",
        "      self.labels_list = labels_list\n",
        "\n",
        "    def Sample(self, sampling_processor, sample_rate = 5, num_samples = 10):\n",
        "      'sampling_processor: object to make sampling'\n",
        "      # Loop through video files in the activity folder\n",
        "      for video_file in os.listdir(self.data_path):\n",
        "        if '.mp4' in video_file:\n",
        "          frames_sampled = []\n",
        "          video_path = os.path.join(self.data_path, video_file)\n",
        "          cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "          frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "          # PROCESSOR FOR FINISHING THE TASK\n",
        "          sampling_processor(cap = cap, frame_count = frame_count,\n",
        "            sample_rate = sample_rate, frames_sampled = frames_sampled,\n",
        "                             num_samples = num_samples)\n",
        "          # POPULATE THE LABEL CORRESPONDING TO THE CURRENT VIDEO\n",
        "          self.labels_list.append(self.maps[video_file])\n",
        "\n",
        "class Feature_extract:\n",
        "  def __init__(self, sampled_type = 'uniform'):\n",
        "    # Load pre-trained ResNet50\n",
        "    self.model = ResNet50(weights='imagenet', include_top=False)\n",
        "    self.sampled_type = sampled_type\n",
        "\n",
        "  # Function to normalize a frame\n",
        "  def normalize_frame(self, frame, ref_mean, ref_std):\n",
        "      actual_mean = np.mean(frame, axis=(0, 1), keepdims=True)\n",
        "      actual_std = np.std(frame, axis=(0, 1))\n",
        "      normalized_frame = (frame - actual_mean) / actual_std * ref_std + ref_mean\n",
        "      return normalized_frame\n",
        "\n",
        "  # Function to preprocess frames and extract features using ResNet\n",
        "  def features(self,frames, ref_mean, ref_std):\n",
        "      processed_frames = [self.normalize_frame(frame, ref_mean, ref_std) for frame in frames]\n",
        "      processed_frames = [preprocess_input(frame) for frame in processed_frames]\n",
        "      features = self.model.predict(np.array(processed_frames))\n",
        "      print(f\"frame: {features.shape}\")\n",
        "      return features\n",
        "\n",
        "class Train_model:\n",
        "  def __init__(self, valid_features_dir, train_features_dir, feaures_base_names = [], labels_base_name = [], **kwargs):\n",
        "    '''kwargs: [val_size ex: 0.2, num_classes: classes of dataset, epoch ex : 200, patience: ex: 10, model_storage_dir: dir to hold trained model]'''\n",
        "    self.trainFeatures = os.path.join(train_features_dir, feaures_base_names[0])\n",
        "    self.valFeatures = os.path.join(valid_features_dir, feaures_base_names[1])\n",
        "    self.trainLabels = os.path.join(train_features_dir, labels_base_name[0])\n",
        "    self.valLabels = os.path.join(valid_features_dir, labels_base_name[1])\n",
        "    self.X_train = np.load(self.trainFeatures, allow_pickle = True)\n",
        "    self.y_train = np.load(self.trainLabels, allow_pickle = True)\n",
        "    self.X_val = np.load(self.valFeatures, allow_pickle = True)\n",
        "    self.y_val = np.load(self.valLabels, allow_pickle = True)\n",
        "\n",
        "    self.kwargs = kwargs\n",
        "    self.val_size = self.kwargs['val_size'] # 0.64\n",
        "    self.shuffle = True\n",
        "    self.num_classes = self.kwargs['num_classes'] #6\n",
        "    self.epoch = self.kwargs['epoch'] #300\n",
        "    self.patience = self.kwargs['patience'] # 10\n",
        "    self.early_stopping = EarlyStopping(monitor='val_loss', patience= self.patience)\n",
        "\n",
        "    features = np.vstack([self.X_train, self.X_val])\n",
        "    labels = np.hstack([self.y_train, self.y_val])\n",
        "    # print(labels.shape)\n",
        "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(features, labels, test_size=self.val_size,\n",
        "                                                                          shuffle = self.shuffle, random_state=42)\n",
        "    # DIR FOR MODEL\n",
        "    # BASE_DIR ='/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/models'\n",
        "    os.makedirs(kwargs['model_storage_dir'], exist_ok = True)\n",
        "\n",
        "  def trainModel(self, model_savename = 'uniform_model'):\n",
        "    # Define the model with 3D convolutional layers\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=self.X_train.shape[1:]),  # Input shape matches your feature shape\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(self.num_classes, activation='softmax')  # Output layer with the number of classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    modelHistory = model.fit(self.X_train, self.y_train, epochs=self.epoch, validation_data=(self.X_val, self.y_val)) # , callbacks=[early_stopping]\n",
        "    plotting(modelHistory)\n",
        "    model.save(os.path.join(self.kwargs['model_storage_dir'],f'{model_savename}.h5'))\n",
        "\n",
        "    # PLOTTING\n",
        "    x = lambda : plotting(modelHistory)\n",
        "    x()\n",
        "\n",
        "  def End2End(self):\n",
        "    # Define the 2D CNN + LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'), input_shape=(self.X_train.shape[1:]))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(LSTM(64, activation='relu'))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(self.X_train, self.y_train, epochs=10, validation_data=(self.X_val, self.y_val))\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = model.evaluate(self.X_val, self.y_val)[1]\n",
        "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "  def model_evaluation(self, val_features, val_labels, model_name):\n",
        "    results = {}\n",
        "    X_val = np.load(val_features, allow_pickle = True)\n",
        "    y_val = np.load(val_labels, allow_pickle = True)\n",
        "    BASE_DIR = self.kwargs['model_storage_dir']\n",
        "    models = os.listdir(BASE_DIR)\n",
        "    # Load your saved model\n",
        "    model = keras.models.load_model(os.path.join(BASE_DIR, f'{model_name}.h5'))\n",
        "    # Evaluate the model on the test data\n",
        "    test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
        "\n",
        "    # Print the evaluation results\n",
        "\n",
        "    print(\"enhanced images sample\")\n",
        "    print(\"-\"*100)\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    print(f'Test Accuracy: {test_accuracy}')\n",
        "    results['test'] = (test_loss, test_accuracy)\n",
        "    # RESULTS DATAFRAME\n",
        "    result_df = pd.DataFrame(results)\n",
        "    print(result_df)\n",
        "    return result_df.to_latex(index=False)\n",
        "\n",
        "def plotting(model):\n",
        "  # Access training history\n",
        "  training_accuracy = model.history['accuracy']\n",
        "  validation_accuracy = model.history['val_accuracy']\n",
        "  training_loss = model.history['loss']\n",
        "  validation_loss = model.history['val_loss']\n",
        "  # Plot the training and validation accuracy\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(training_accuracy, label='Training Accuracy')\n",
        "  plt.plot(validation_accuracy, label='Validation Accuracy')\n",
        "  plt.plot(training_loss, label='Training loss')\n",
        "  plt.plot(validation_loss, label='Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training and Validation')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def display_frames(frames, title):\n",
        "    num_frames = len(frames)\n",
        "    fig, axs = plt.subplots(1, num_frames, figsize=(15, 3))\n",
        "\n",
        "    for ax, (i, frame) in zip(axs, frames.items()):\n",
        "        ax.imshow(frame)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'{i}')\n",
        "\n",
        "    # Adjust layout to minimize space between subplots\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#object construction"
      ],
      "metadata": {
        "id": "uozieIbBPRNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_file = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate.txt'\n",
        "samp = Sampling(video_path = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train',\n",
        "                map_file = map_file,\n",
        "                sampling_type='random',\n",
        "                val_sampling = False,\n",
        "                ref_mean=[0.07, 0.07, 0.07],\n",
        "                ref_std=[0.1, 0.09, 0.08],\n",
        "                save_feature_dir = '/content/drive/MyDrive/dummy',enhance_img = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Rp2xoLAuPV96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_features_dir = '/content/drive/MyDrive/dummy'\n",
        "train_features_dir = '/content/drive/MyDrive/dummy'\n",
        "feaures_base_names = ['features1.npy','features2.npy']\n",
        "labels_base_name = ['labels1.npy','labels2.npy']\n",
        "classifier = Train_model(valid_features_dir, train_features_dir,\n",
        "                         feaures_base_names, labels_base_name,\n",
        "                         val_size = 0.2,\n",
        "                         num_classes = 6,\n",
        "                         epoch = 10,\n",
        "                         patience = 10,\n",
        "                         model_storage_dir = '/content/drive/MyDrive/docx')\n",
        "classifier.trainModel(model_savename = 'mode')"
      ],
      "metadata": {
        "id": "2A3x3MocydWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#end-to-end"
      ],
      "metadata": {
        "id": "SAyGnyOLcp8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "# Specify the height and width to which each video frame will be resized in our dataset.\n",
        "img_h , img_w = 64, 64\n",
        "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
        "sizeOfSequence = 20\n",
        "# Specify the directory containing the UCF50 dataset.\n",
        "dir = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train'\n",
        "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
        "classes_to_predict = [\"Jump\", \"Run\", \"Sit\", \"Stand\", 'Walk', 'Turn']\n",
        "\n",
        "def Sampling(video_path):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_collected: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    # Declare a list to store video frames.\n",
        "    frames_collected = []\n",
        "\n",
        "    # Read the Video File using the VideoCapture object.\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the total number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/sizeOfSequence), 1)\n",
        "\n",
        "    # Iterate through the Video Frames.\n",
        "    for frame_counter in range(sizeOfSequence):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video.\n",
        "        rec, frame = video_reader.read()\n",
        "\n",
        "        # Check if Video frame is not recfully read then break the loop\n",
        "        if not rec:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed height and width.\n",
        "        resized_frame = cv2.resize(frame, (img_h, img_w))\n",
        "\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        # Append the normalized frame into the frames list\n",
        "        frames_collected.append(normalized_frame)\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "    # Return the frames list.\n",
        "    return frames_collected\n",
        "\n",
        "def prepareDataset():\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "        video_files_paths: A list containing the paths of the videos in the disk.\n",
        "    '''\n",
        "\n",
        "    # Declared Empty Lists to store the features, labels and video file path values.\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "\n",
        "    # Iterating through all the classes mentioned in the classes list\n",
        "    for index_in_class, class_ in enumerate(classes_to_predict):\n",
        "        # Get the list of video files present in the specific class name directory.\n",
        "        files = os.listdir(os.path.join(dir, class_))\n",
        "\n",
        "        # Iterate through all the files present in the files list.\n",
        "        for file in files:\n",
        "\n",
        "            # Get the complete video path.\n",
        "            video_file_path = os.path.join(dir, class_, file)\n",
        "            frames = Sampling(video_file_path)\n",
        "\n",
        "            # Check if the extracted frames are equal to the sizeOfSequence specified above.\n",
        "            # So ignore the vides having frames less than the sizeOfSequence.\n",
        "            if len(frames) == sizeOfSequence:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(index_in_class)\n",
        "                video_files_paths.append(video_file_path)\n",
        "\n",
        "    # Converting the list to numpy arrays\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Return the frames, class index, and video file path.\n",
        "    return features, labels, video_files_paths\n",
        "\n",
        "# Create the dataset.\n",
        "features, labels, video_files_paths = prepareDataset()\n",
        "\n",
        "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
        "one_hot_encoded_labels = to_categorical(labels)\n",
        "\n",
        "# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,\n",
        "                                                                            test_size = 0.25, shuffle = True,\n",
        "                                                                            random_state = seed)\n",
        "\n",
        "def create_convlstm_model():\n",
        "    '''\n",
        "    This function will construct the required convlstm model.\n",
        "    Returns:\n",
        "        model: It is the required constructed convlstm model.\n",
        "    '''\n",
        "\n",
        "    # We will use a Sequential model for model construction\n",
        "    model = Sequential()\n",
        "\n",
        "    # Define the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "\n",
        "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (sizeOfSequence,\n",
        "                                                                                      img_h, img_w, 3)))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "\n",
        "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "\n",
        "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "\n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    #model.add(TimeDistributed(Dropout(0.2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(len(classes_to_predict), activation = \"softmax\"))\n",
        "\n",
        "    ########################################################################################################################\n",
        "\n",
        "    # Display the models summary.\n",
        "    model.summary()\n",
        "\n",
        "    # Return the constructed convlstm model.\n",
        "    return model\n",
        "\n",
        "# Construct the required convlstm model.\n",
        "convlstm_model = create_convlstm_model()\n",
        "\n",
        "# Display the rec message.\n",
        "print(\"Model Created recfully!\")\n",
        "\n",
        "# Plot the structure of the contructed model.\n",
        "plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "# Create an Instance of Early Stopping Callback\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 8, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "# Compile the model and specify loss function, optimizer and metrics values to the model\n",
        "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        "\n",
        "# Start training the model.\n",
        "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 25, batch_size = 4,\n",
        "                                                     shuffle = True, validation_split = 0.2,\n",
        "                                                     callbacks = [early_stopping_callback])\n",
        "\n",
        "# Evaluate the trained model.\n",
        "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)\n",
        "\n",
        "# Get the loss and accuracy from model_evaluation_history.\n",
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "# Define the string date format.\n",
        "# Get the current Date and Time in a DateTime Object.\n",
        "# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "\n",
        "# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
        "model_file = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        "\n",
        "# Save your Model.\n",
        "convlstm_model.save(model_file)\n",
        "\n",
        "\n",
        "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
        "    '''\n",
        "    This function will plot the metrics passed to it in a graph.\n",
        "    Args:\n",
        "        model_training_history: A history object containing a record of training and validation\n",
        "                                loss values and metrics values at recive epochs\n",
        "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
        "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
        "        plot_name:              The title of the graph.\n",
        "    '''\n",
        "\n",
        "    # Get metric values using metric names as identifiers.\n",
        "    metric_value_1 = model_training_history.history[metric_name_1]\n",
        "    metric_value_2 = model_training_history.history[metric_name_2]\n",
        "\n",
        "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
        "    epochs = range(len(metric_value_1))\n",
        "\n",
        "    # Plot the Graph.\n",
        "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        "\n",
        "    # Add title to the plot.\n",
        "    plt.title(str(plot_name))\n",
        "\n",
        "    # Add legend to the plot.\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "# Visualize the training and validation loss metrices.\n",
        "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')\n",
        "\n",
        "# Visualize the training and validation accuracy metrices.\n",
        "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')\n",
        "\n",
        "# Implement the LRCN Approach\n",
        "def create_LRCN_model():\n",
        "    '''\n",
        "    This function will construct the required LRCN model.\n",
        "    Returns:\n",
        "        model: It is the required constructed LRCN model.\n",
        "    '''\n",
        "\n",
        "    # We will use a Sequential model for model construction.\n",
        "    model = Sequential()\n",
        "\n",
        "    # Define the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
        "                              input_shape = (sizeOfSequence, img_h, img_w, 3)))\n",
        "\n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    #model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(LSTM(32))\n",
        "\n",
        "    model.add(Dense(len(classes_to_predict), activation = 'softmax'))\n",
        "\n",
        "    ########################################################################################################################\n",
        "\n",
        "    # Display the models summary.\n",
        "    model.summary()\n",
        "\n",
        "    # Return the constructed LRCN model.\n",
        "    return model\n",
        "\n",
        "# Construct the required LRCN model.\n",
        "LRCN_model = create_LRCN_model()\n",
        "\n",
        "# Display the rec message.\n",
        "print(\"Model Created recfully!\")\n",
        "\n",
        "# Plot the structure of the contructed LRCN model.\n",
        "plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "\n",
        "# Create an Instance of Early Stopping Callback.\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "# Compile the model and specify loss function, optimizer and metrics to the model.\n",
        "LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        "\n",
        "# Start training the model.\n",
        "LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 70, batch_size = 4 ,\n",
        "                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])\n",
        "\n",
        "\n",
        "# Evaluate the trained model.\n",
        "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)\n",
        "\n",
        "\n",
        "# Get the loss and accuracy from model_evaluation_history.\n",
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "# Define the string date format.\n",
        "# Get the current Date and Time in a DateTime Object.\n",
        "# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "\n",
        "# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
        "model_file = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        "\n",
        "# Save the Model.\n",
        "LRCN_model.save(model_file)\n",
        "\n",
        "\n",
        "# Visualize the training and validation loss metrices.\n",
        "plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')\n",
        "\n",
        "# Visualize the training and validation accuracy metrices.\n",
        "plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')\n",
        "\n",
        "# Create a Function To Perform Action Recognition on Videos\n",
        "input_video_file_path = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/240.mp4'\n",
        "\n",
        "\n",
        "def predict_on_video(video_file_path, output_file_path, sizeOfSequence):\n",
        "    '''\n",
        "    This function will perform action recognition on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n",
        "    sizeOfSequence:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        "\n",
        "    # Initialize the VideoCapture object to read from the video file.\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Get the width and height of the video.\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Initialize the VideoWriter Object to store the output video in the disk.\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "\n",
        "    # Declare a queue to store video frames.\n",
        "    frames_queue = deque(maxlen = sizeOfSequence)\n",
        "\n",
        "    # Initialize a variable to store the predicted action being performed in the video.\n",
        "    predicted_class_ = ''\n",
        "\n",
        "    # Iterate until the video is accessed recfully.\n",
        "    while video_reader.isOpened():\n",
        "\n",
        "        # Read the frame.\n",
        "        ok, frame = video_reader.read()\n",
        "\n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed Dimensions.\n",
        "        resized_frame = cv2.resize(frame, (img_h, img_w))\n",
        "\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        # Appending the pre-processed frame into the frames list.\n",
        "        frames_queue.append(normalized_frame)\n",
        "\n",
        "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
        "        if len(frames_queue) == sizeOfSequence:\n",
        "\n",
        "            # Pass the normalized frames to the model and get the predicted probabilities.\n",
        "            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
        "\n",
        "            # Get the index of class with highest probability.\n",
        "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "\n",
        "            # Get the class name using the retrieved index.\n",
        "            predicted_class_ = classes_to_predict[predicted_label]\n",
        "\n",
        "        # Write predicted class name on top of the frame.\n",
        "        cv2.putText(frame, predicted_class_, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write The frame into the disk using the VideoWriter Object.\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the VideoCapture and VideoWriter objects.\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "\n",
        "# Construct the output video path.\n",
        "output_video_file_path = f'Output-SeqLen{sizeOfSequence}.mp4'\n",
        "\n",
        "# Perform Action Recognition on the Test Video.\n",
        "predict_on_video(input_video_file_path, output_video_file_path, sizeOfSequence)\n",
        "\n",
        "# Display the output video.\n",
        "VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()\n",
        "\n",
        "\n",
        "def predict_single_action(video_file_path, sizeOfSequence):\n",
        "    '''\n",
        "    This function will perform single action recognition prediction on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    sizeOfSequence:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        "\n",
        "    # Initialize the VideoCapture object to read from the video file.\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Get the width and height of the video.\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Declare a list to store video frames we will extract.\n",
        "    frames_collected = []\n",
        "\n",
        "    # Initialize a variable to store the predicted action being performed in the video.\n",
        "    predicted_class_ = ''\n",
        "\n",
        "    # Get the number of frames in the video.\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/sizeOfSequence),1)\n",
        "\n",
        "    # Iterating the number of times equal to the fixed length of sequence.\n",
        "    for frame_counter in range(sizeOfSequence):\n",
        "\n",
        "        # Set the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Read a frame.\n",
        "        rec, frame = video_reader.read()\n",
        "\n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not rec:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed Dimensions.\n",
        "        resized_frame = cv2.resize(frame, (img_h, img_w))\n",
        "\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        # Appending the pre-processed frame into the frames list\n",
        "        frames_collected.append(normalized_frame)\n",
        "\n",
        "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
        "    predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_collected, axis = 0))[0]\n",
        "\n",
        "    # Get the index of class with highest probability.\n",
        "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "\n",
        "    # Get the class name using the retrieved index.\n",
        "    predicted_class_ = classes_to_predict[predicted_label]\n",
        "\n",
        "    # Display the predicted action along with the prediction confidence.\n",
        "    print(f'Action Predicted: {predicted_class_}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n",
        "\n",
        "    # Release the VideoCapture object.\n",
        "    video_reader.release()\n",
        "\n",
        "# Perform Single Prediction on the Test Video.\n",
        "predict_single_action(input_video_file_path, sizeOfSequence)\n",
        "\n",
        "# Display the input video.\n",
        "VideoFileClip(input_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()"
      ],
      "metadata": {
        "id": "CQ8GZ75NfFeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#version 2"
      ],
      "metadata": {
        "id": "zko1lGvMjiyj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYUVeNbIjkmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}